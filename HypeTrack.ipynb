{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7838dea-4741-4d6e-8d73-119c937abe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries \n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import tweepy\n",
    "import re\n",
    "import csv\n",
    "import asyncio\n",
    "import time\n",
    "import openpyxl\n",
    "import nest_asyncio\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from telethon.sync import TelegramClient\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import NamedStyle\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f29cf3-b840-4665-90e5-312dca2c04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X's API Initialization\n",
    "Bearer_Token = \"AAAAAAAAAAAAAAAAAAAAAP2%2BzgEAAAAAcesSwufQuNkNph4oM3%2BPI1tQoiM%3DfZnoWNQJpEN1OXvrAKglj5PZYGaHtbQWedIjjBgOmIhkTxmTCb\"\n",
    "API_Key = \"eXHKcdVzjeLi8CLbdeDZ1MWpy\"\n",
    "X_KeySecret = \"hoyHQeyOtbLSAWQgnueNpd2XASOycVUo1utZ8sThdZ9SsKxRJc\"\n",
    "X_Token = \"807562185681633282-pQEABmamoUon6wv2FG4aotgGpzTleTI\"\n",
    "X_AccessToken = \"bV0dmHTIWkZ0mkfazL1Mqp5Uc9jQO4N89ybNp4k9S9jHv\"\n",
    "\n",
    "#Authentication with OAuth\n",
    "auth = tweepy.OAuth1UserHandler(API_Key, X_KeySecret,X_Token,X_AccessToken)\n",
    "api = tweepy.API(auth)\n",
    "user = tweepy.Client(bearer_token=Bearer_Token)\n",
    "\n",
    "# Check authentication\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Twitter API authentication successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Authentication error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d5dcc-69a4-4cd7-a2b4-6027a50ab37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Telegram credentials\n",
    "API_ID = \"26137609\"\n",
    "API_Hash = \"837e683dbc20e9b13f368688f00954ed\"\n",
    "Phone_number = \"+2348027343812\"\n",
    "\n",
    "client = TelegramClient(\"Scrape\", API_ID, API_Hash)\n",
    "async def main():\n",
    "    await client.start()\n",
    "    me = await client.get_me()  # Fetch user info to confirm login\n",
    "    print(f\"✅ Authentication successful! Logged in as: {me.username} ({me.id})\")\n",
    "\n",
    "try:\n",
    "    asyncio.get_running_loop()\n",
    "    print(\"Running inside an event loop, using 'await main()' instead...\")\n",
    "    await main()\n",
    "except RuntimeError:\n",
    "    print(\"Running normally with asyncio.run()\")\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bceecd2a-7459-485b-8aaa-cf3676ad5f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File selected: C:/Users/FAVOUR ANAZODO/Documents/Python Projects/SHILL SQUAD.xlsx\n"
     ]
    }
   ],
   "source": [
    "#Open a file dialog\n",
    "source = tk.Tk()\n",
    "source.withdraw() #Hide the root window\n",
    "source.update()\n",
    "\n",
    "file_path = filedialog.askopenfilename(title=\"Select an Excel file\", filetypes=[(\"Excel files\", \"*.xlsx\")])\n",
    "if file_path:\n",
    "    print(f\"File selected: {file_path}\")\n",
    "else:\n",
    "    print(\"No file selected. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "# Load the Excel file\n",
    "wb = load_workbook(file_path)\n",
    "default_style = NamedStyle(name=\"default\")\n",
    "wb.add_named_style(default_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf9d171-512d-4705-b27f-16606041527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract hyperlinks from a cell\n",
    "def hidden_hyperlink(cell):\n",
    "    if cell.hyperlink:\n",
    "        return cell.hyperlink.target  # Extract hyperlink if present\n",
    "    return None\n",
    "\n",
    "#Link Counts\n",
    "X_Count = 0\n",
    "telegram_count = 0\n",
    "debank_count = 0\n",
    "other_count\n",
    "\n",
    "# === TWITTER (X) API SECTION ===\n",
    "def fetch_tweets(tweet_ids):\n",
    "    \"\"\"\n",
    "    Fetch engagement data for multiple tweets in a batch request.\n",
    "    \"\"\"\n",
    "        \n",
    "    results = {}\n",
    "    batch_size = 100  # Twitter API allows max 100 tweet IDs per request\n",
    "\n",
    "    for i in range(0, len(tweet_ids), batch_size):\n",
    "        batch = tweet_ids[i:i + batch_size]  # Split into chunks of 100\n",
    "        try:\n",
    "            response = user.get_tweets(ids=batch, tweet_fields=[\"public_metrics\"])\n",
    "            \n",
    "            if response.data:\n",
    "                for tweet in response.data:\n",
    "                    metrics = tweet.public_metrics\n",
    "                    results[str(tweet.id)] = {\n",
    "                        \"likes\": metrics[\"like_count\"],\n",
    "                        \"retweets\": metrics[\"retweet_count\"],\n",
    "                        \"replies\": metrics[\"reply_count\"],\n",
    "                    }\n",
    "            \n",
    "            # ✅ Respect Rate Limits (Sleep for 1 second between batches)\n",
    "            time.sleep(1)\n",
    "        \n",
    "        except tweepy.TooManyRequests:\n",
    "            print(\"⚠️ Rate limit exceeded. Sleeping for 15 minutes...\")\n",
    "            time.sleep(15 * 60)  # Wait for 15 minutes before retrying\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching tweets: {e}\")\n",
    "    \n",
    "    return results  # Returns {tweet_id: {likes, retweets, replies}}\n",
    "\n",
    "def X_data(url):\n",
    "    \"\"\"Extract Tweet ID and fetch engagement data.\"\"\"\n",
    "    match = re.search(r'status/(\\d+)', url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# === TELEGRAM API SECTION ===\n",
    "async def fetch_telegram_data(channel_username, message_id):\n",
    "    \"\"\"Fetch views, reactions, and forwards for a given Telegram post.\"\"\"\n",
    "    try:\n",
    "        message = await client.get_messages(channel_username, ids=message_id)\n",
    "        return {\n",
    "            \"views\": message.views if message.views else 0,\n",
    "            \"reactions\": message.reactions.to_dict() if message.reactions else {},\n",
    "            \"forwards\": message.forwards if message.forwards else 0\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching Telegram data: {e}\"\n",
    "\n",
    "def telegram_data(url):\n",
    "    \"\"\"Extract Telegram message info and fetch engagement data asynchronously.\"\"\"\n",
    "    match = re.search(r\"t\\.me/([^/]+)/(\\d+)\", url)\n",
    "    if match:\n",
    "        channel_username = match.group(1)\n",
    "        message_id = int(match.group(2))\n",
    "\n",
    "        try:\n",
    "            loop = asyncio.get_running_loop()  # ✅ Get the running event loop\n",
    "            return asyncio.ensure_future(fetch_telegram_data(channel_username, message_id))  # ✅ Schedule task\n",
    "        except RuntimeError:\n",
    "            return asyncio.run(fetch_telegram_data(channel_username, message_id))  # ✅ If no loop, start one\n",
    "\n",
    "    return \"Invalid Telegram URL\"\n",
    "\n",
    "\n",
    "# === SCRAPING PROCESS ===\n",
    "# Store tweet IDs and async Telegram tasks\n",
    "tweet_ids = []\n",
    "telegram_tasks = []\n",
    "all_data = []\n",
    "\n",
    "for sheet_name in wb.sheetnames:\n",
    "    sheet = wb[sheet_name]\n",
    "    \n",
    "    # Process each row\n",
    "    for row in sheet.iter_rows(min_row=2, values_only=False):  \n",
    "        first_col_value = row[0].value\n",
    "        for cell in row:\n",
    "            url = hidden_hyperlink(cell)\n",
    "            if url:\n",
    "                if \"x.com\" in url:\n",
    "                    tweet_id = X_data(url)\n",
    "                    if tweet_id:\n",
    "                        tweet_ids.append(tweet_id)\n",
    "                        all_data.append([first_col_value, url, tweet_id])  # Store tweet_id for batch processing\n",
    "                elif \"t.me\" in url or \"telegram.me\" in url:\n",
    "                    task = telegram_data(url)\n",
    "                    if task:\n",
    "                        telegram_tasks.append(task)\n",
    "                        all_data.append([first_col_value, url, task])  # Store task for async execution\n",
    "                else:\n",
    "                    all_data.append([first_col_value, url, \"Unsupported website\"])\n",
    "\n",
    "# === FETCH DATA IN BATCHES ===\n",
    "# 1️⃣ Fetch Twitter Data in Batch\n",
    "if tweet_ids:\n",
    "    twitter_results = fetch_tweets(tweet_ids)  # Call the function directly, do NOT wrap in asyncio task\n",
    "    for row in all_data:\n",
    "        if isinstance(row[2], str) and row[2] in twitter_results:  # ✅ Ensure it's a valid string\n",
    "            engagement = twitter_results[row[2]]  # ✅ Get the dictionary\n",
    "            row[2] = f\"Likes: {engagement['likes']}, Retweets: {engagement['retweets']}, Replies: {engagement['replies']}\"\n",
    "\n",
    "# 2️⃣ Fetch Telegram Data in Parallel\n",
    "if telegram_tasks:\n",
    "    telegram_results = await asyncio.gather(*telegram_tasks)\n",
    "    task_index = 0\n",
    "    for row in all_data:\n",
    "        if isinstance(row[2], asyncio.Future):  # Identify async tasks\n",
    "            row[2] = telegram_results[task_index]  # Replace task with actual results\n",
    "            task_index += 1\n",
    "\n",
    "# === SAVE TO CSV ===\n",
    "output_csv = \"scraped_data.csv\"\n",
    "with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Name\", \"Link\", \"Scraped Data\"])\n",
    "    writer.writerows(all_data)\n",
    "\n",
    "print(f\"✅ Scraping completed. Data saved in {output_csv}\")\n",
    "\n",
    "# Disconnect Telegram Client\n",
    "client.disconnect()\n",
    "\n",
    "#Run Analysis\n",
    "#Load CSV\n",
    "df = pd.read_csv(\"scraped_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ccdf2-f678-4406-a758-7b7640b497d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "excel_workbook = \"SHILL SQUAD.xlsx\"\n",
    "wb = load_workbook(excel_workbook)\n",
    "default_style = NamedStyle(name=\"default\")\n",
    "wb.add_named_style(default_style)\n",
    "\n",
    "# Function to extract hyperlinks from a cell\n",
    "def hidden_hyperlink(cell):\n",
    "    if cell.hyperlink:\n",
    "        return cell.hyperlink.target  # Extract hyperlink if present\n",
    "    return None  # Return None if no hyperlink\n",
    "\n",
    "# Function to batch fetch tweet engagement data\n",
    "def fetch_tweet_engagement(tweet_ids):\n",
    "    try:\n",
    "        response = user.lookup_statuses(tweet_ids, tweet_mode=\"extended\")  # Correct method\n",
    "        results = {}\n",
    "\n",
    "        for tweet in tweets:\n",
    "            results[tweet.id] = {\n",
    "                \"likes\": tweet.favorite_count,\n",
    "                \"retweets\": tweet.retweet_count,\n",
    "                \"replies\": tweet.reply_count if hasattr(tweet, \"reply_count\") else 0,\n",
    "            }\n",
    "        return results    # Ensure we return a dictionary\n",
    "    except tweepy.TooManyRequests as e:\n",
    "        print(f\"Rate limit exceeded. Sleeping for {e.retry_after} seconds...\")\n",
    "        time.sleep(e.retry_after)  # Sleep only for required time\n",
    "        return fetch_tweet_engagement(tweet_ids)  # Retry after sleep\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching tweets: {e}\")\n",
    "        return {}    # Return an empty dictionary instead of a string\n",
    "\n",
    "def X_data(url):\n",
    "    match = re.search(r'status/(\\d+)', url)\n",
    "    if match:\n",
    "        tweet_id = match.group(1)\n",
    "        try:\n",
    "            time.sleep(3)  # Wait 3 seconds between requests\n",
    "            tweet = user.get_tweet(tweet_id, tweet_fields=[\"public_metrics\"])\n",
    "            if tweet.data:\n",
    "                metrics = tweet.data[\"public_metrics\"]\n",
    "                return {\n",
    "                    \"likes\": metrics[\"like_count\"],\n",
    "                    \"retweets\": metrics[\"retweet_count\"],\n",
    "                    \"replies\": metrics[\"reply_count\"],\n",
    "                }\n",
    "            else:\n",
    "                return \"Tweet not found\"\n",
    "        except Exception as e:\n",
    "            return f\"Error fetching tweet: {e}\"\n",
    "    return \"Invalid URL\"\n",
    "    \n",
    "# Async function to fetch Telegram engagement data\n",
    "async def fetch_telegram_data(channel_username, message_id):\n",
    "    return await client.get_messages(channel_username, ids=message_id)\n",
    "\n",
    "# Function to get Telegram data\n",
    "def telegram_data(url):\n",
    "    match = re.search(r\"t\\.me/([^/]+)/(\\d+)\", url)  # Extract channel & message ID\n",
    "    if match:\n",
    "        channel_username = match.group(1)\n",
    "        message_id = int(match.group(2))\n",
    "\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            if loop.is_running():\n",
    "                # If event loop is already running, create a new task\n",
    "                message = asyncio.ensure_future(fetch_telegram_data(channel_username, message_id))\n",
    "                return message.result()  # Wait for the result\n",
    "            else:\n",
    "                # If no event loop, run normally\n",
    "                return loop.run_until_complete(fetch_telegram_data(channel_username, message_id))\n",
    "        except Exception as e:\n",
    "            return f\"Error fetching Telegram data: {e}\"\n",
    "    return \"Invalid Telegram URL\"\n",
    "\n",
    "# Function to determine scraping method\n",
    "def scrape_data(url):\n",
    "    if \"x.com\" in url:\n",
    "        return X_data(url)\n",
    "    elif \"t.me\" in url or \"telegram.me\" in url:\n",
    "        return telegram_data(url)\n",
    "    else:\n",
    "        return \"Unsupported website\"\n",
    "\n",
    "# Process each sheet in the Excel file\n",
    "all_data = []\n",
    "for sheet_name in wb.sheetnames:\n",
    "    sheet = wb[sheet_name]\n",
    "    \n",
    "    # Extract headers (first row)\n",
    "    headers = [cell.value for cell in sheet[1]]\n",
    "    \n",
    "    # Process each row\n",
    "    for row in sheet.iter_rows(min_row=2, values_only=False):  # Skip headers\n",
    "        row_data = [cell.value for cell in row]  # Extract normal values\n",
    "        first_col_value = row[0].value  # Store first column value\n",
    "        \n",
    "        # Find and process hyperlinks\n",
    "        for cell in row:\n",
    "            url = hidden_hyperlink(cell)\n",
    "            if url:\n",
    "                scraped_info = scrape_data(url)\n",
    "                all_data.append([first_col_value, url, scraped_info])  # Maintain first column value\n",
    "\n",
    "# Write the output to a CSV file\n",
    "output_csv = \"scraped_data.csv\"\n",
    "with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"First Column\", \"Link\", \"Scraped Data\"])  # Header\n",
    "    writer.writerows(all_data)\n",
    "\n",
    "print(f\"Scraping completed. Data saved in {output_csv}\")\n",
    "\n",
    "# Disconnect Telegram Client\n",
    "client.disconnect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8c985-697d-4222-8a60-05fa4d6af39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract hyperlinks from excel cell\n",
    "def hidden_hyperlink(sheet):\n",
    "    links = []\n",
    "    for row in sheet.iter_rows():\n",
    "        for cell in row:\n",
    "            if cell.hyperlink:\n",
    "                links.append((cell.row, cell.column, cell.hyperlink.target))\n",
    "    return links\n",
    "\n",
    "# Function to scrape Twitter data\n",
    "def X_data(url):\n",
    "    match = re.search(r'status/(\\d+)', url)\n",
    "    if match:\n",
    "        tweet_id = match.group(1)\n",
    "        try:\n",
    "            tweet = X.show_status(id=tweet_id)\n",
    "            return tweet  # Returns full tweet data\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching tweet: {e}\")\n",
    "    return None\n",
    "\n",
    "# Function to scrape Telegram data\n",
    "def scrape_telegram(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Example: Extract title of Telegram page (modify as needed)\n",
    "        title = soup.find(\"title\")\n",
    "        return title.text if title else \"No data found\"\n",
    "    except Exception as e:\n",
    "        return f\"Error scraping Telegram: {e}\"\n",
    "\n",
    "\n",
    "def process_excel(file_path):\n",
    "#load excel sheets\n",
    "    workbook = openpyxl.load_workbook(file_path)\n",
    "    output_data = []\n",
    "#parse through sheets    \n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "        hyperlinks = hidden_hyperlinks(sheet)\n",
    "#extract based on link        \n",
    "        for row, col, url in hyperlinks:\n",
    "            if \"twitter.com\" in url:\n",
    "                tweet_data = X_data(url)\n",
    "                if tweet_data:\n",
    "                    output_data.append([sheet.cell(row, 1).value, sheet.cell(1, col).value, tweet_data])\n",
    "            elif \"t.me\" in url:\n",
    "                telegram_data = Telegram_data(url)\n",
    "                if telegram_data:\n",
    "                    output_data.append([sheet.cell(row, 1).value, sheet.cell(1, col).value, telegram_data])\n",
    "            else:\n",
    "                output_data.append([sheet.cell(row, 1).value, sheet.cell(1, col).value, f\"Other link: {url}\"])\n",
    "    \n",
    "    df = pd.DataFrame(output_data, columns=[\"First Column\", \"First Row\", \"Extracted Data\"])\n",
    "    df.to_csv(\"output.csv\", index=False)\n",
    "    print(\"Data extraction complete. Saved to output.csv\")\n",
    "\n",
    "# Run process\n",
    "process_excel(\"your_excel_file.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6db5e9-25f9-4fc4-9055-0bf37fe20255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to determine scraping method\n",
    "def scrape_data(url):\n",
    "    if \"twitter.com\" in url:\n",
    "        return scrape_twitter(url)\n",
    "    elif \"t.me\" in url or \"telegram.me\" in url:\n",
    "        return scrape_telegram(url)\n",
    "    else:\n",
    "        return \"Unsupported website\"\n",
    "\n",
    "# Process each sheet in the Excel file\n",
    "all_data = []\n",
    "for sheet_name in wb.sheetnames:\n",
    "    sheet = wb[sheet_name]\n",
    "    \n",
    "    # Extract headers (first row)\n",
    "    headers = [cell.value for cell in sheet[1]]\n",
    "    \n",
    "    # Process each row\n",
    "    for row in sheet.iter_rows(min_row=2, values_only=False):  # Skip headers\n",
    "        row_data = [cell.value for cell in row]  # Extract normal values\n",
    "        first_col_value = row[0].value  # Store first column value\n",
    "        \n",
    "        # Find and process hyperlinks\n",
    "        for cell in row:\n",
    "            url = get_hyperlink(cell)\n",
    "            if url:\n",
    "                scraped_info = scrape_data(url)\n",
    "                all_data.append([first_col_value, url, scraped_info])  # Maintain first column value\n",
    "\n",
    "# Write the output to a CSV file\n",
    "output_csv = \"scraped_data.csv\"\n",
    "with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"First Column\", \"Link\", \"Scraped Data\"])  # Header\n",
    "    writer.writerows(all_data)\n",
    "\n",
    "print(f\"Scraping completed. Data saved in {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f26c30-097c-4393-8e65-6cfea4d59ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "#Telegram credentials\n",
    "API_ID = \"26137609\"\n",
    "API_Hash = \"837e683dbc20e9b13f368688f00954ed\"\n",
    "Phone_number = \"+2348027343812\"\n",
    "\n",
    "#Authentication\n",
    "\n",
    "\n",
    "async def check_auth():\n",
    "    async with TelegramClient(\"Scrape\", API_ID, API_Hash) as client:\n",
    "        me = await client.get_me()  # Get account info\n",
    "        if me:\n",
    "            print(f\"Authentication successful! Logged in as: {me.username} ({me.id})\")\n",
    "        else:\n",
    "            print(\"Authentication failed!\")\n",
    "\n",
    "asyncio.run(check_auth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220e558-0cba-4c4c-95bc-170b26559924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
